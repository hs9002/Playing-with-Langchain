{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing API Keys\n",
    "import os\n",
    "import openai\n",
    "import langchain\n",
    "\n",
    "# Store openai API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY HERE\"\n",
    "\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate model object and generate chat completions\n",
    "llm = OpenAI()\n",
    "res = llm.invoke(\"How much time is needed to become an advanced user of Python?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time it takes to become an advanced user of Python can vary greatly depending on an individual's learning style, dedication, and prior programming experience. However, on average, it can take anywhere from 6 months to 2 years to become an advanced user of Python. This time frame assumes consistent practice and learning, as well as completing various projects and exercises to gain practical experience with the language. Additionally, attending workshops, taking online courses, and participating in coding challenges can also help speed up the learning process.\n"
     ]
    }
   ],
   "source": [
    "# PARAMTER CONTROLS\n",
    "# We can set temperature (between 0 and 1). This moderates randomness. For focused answers set this close to 0. \n",
    "# For more creative/random answers set this close to 1.\n",
    "\n",
    "llm = OpenAI(temperature=0.5)\n",
    "res_temp = llm.invoke(\"How much time is needed to become an advanced user of Python?\")\n",
    "\n",
    "print(res_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The amount of time it takes to become an advanced user of Python can vary depending on a person's prior knowledge and experience with programming languages, their learning style, and the amount of time they dedicate to learning and practicing Python. On average, it may take anywhere from 6 months to 2 years to become an advanced user of Python, but it's important to note that learning is an ongoing process and there is always more to learn in the dynamic field of programming. Consistent practice and projects can greatly accelerate the learning process.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=1)\n",
    "res_temp = llm.invoke(\"How much time is needed to become an advanced user of Python?\")\n",
    "\n",
    "print(res_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter controls - if you want to build chat bot, might want to consider 'streaming' and 'callbacks' parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='나는 파이썬 슈퍼 유저가 되고 싶어요.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 32, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5ea16847-dd0b-4778-af24-72f3dff7595e-0'\n",
      "나는 파이썬 슈퍼 유저가 되고 싶어요.\n"
     ]
    }
   ],
   "source": [
    "# Lets play with SystemMessage. What this does is give ChatGPT a 'role'. \n",
    "# To handle messages like SystemMessage, HumanMessage, and AIMessage, you need to use ChatOpenAI, which is specifically designed for managing conversational inputs.\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai_set = ChatOpenAI(temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful translater that translates English to Korean.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"I want to become a Python superuser.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response_out = openai_set(messages)\n",
    "\n",
    "# AIMessage output: \n",
    "print(response_out)\n",
    "\n",
    "# To see just the response (i.e. just the text)\n",
    "print(response_out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's be more creative \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, streaming = True)\n",
    "\n",
    "answer = chat(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a philosopher. When you get a question you answer it methaphorically and explain your thinking process.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=\"What does it mean to live.\"\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Living is like being a drop of water in a vast ocean. Each drop is unique and contributes to the overall essence of the ocean. Similarly, each individual life is distinct and adds to the richness of the world. To live is to experience the ebb and flow of existence, to interact with others, and to leave a ripple effect that extends far beyond our own understanding.\n",
      "\n",
      "My thinking process here was to use the metaphor of a drop of water in an ocean to convey the interconnectedness of life and the idea that each individual life has value and significance in the grand scheme of things. By comparing living to a drop of water, I aimed to capture the essence of existence as a part of a larger whole, where our actions and interactions have a lasting impact on the world around us.\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Templates**\n",
    "\n",
    "There are basically two templates: \n",
    "1. Prompt Template\n",
    "2. Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about PUBG')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "string_prompt = PromptTemplate.from_template(\"Tell me a joke about {subject}\")\n",
    "string_prompt_value = string_prompt.format_prompt(subject=\"PUBG\")\n",
    "\n",
    "string_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_string() function to turn into raw text \n",
    "print(string_prompt_value.to_string())\n",
    "\n",
    "# Same for the ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try to use PromptTemplate to come up with some recipes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate   \n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template_cooking = \"\"\"\n",
    "You are a fine dining chef. Recommend me one dish I can make with only the ingredients that I have, and explain how I can make it. \n",
    "The ingredients I have are: \n",
    "{ingredients}\n",
    "\"\"\"\n",
    "\n",
    "p_template = PromptTemplate(\n",
    "    input_variables=['ingredients'], \n",
    "    template = template_cooking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a fine dining chef. Recommend me one dish I can make with only the ingredients that I have, and explain how I can make it. \n",
      "The ingredients I have are: \n",
      "Onion, Eggs, Butter, Bacon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(p_template.format(ingredients= 'Onion, Eggs, Butter, Bacon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend making a classic French dish called Quiche Lorraine. Here's how you can make it:\n",
      "\n",
      "Ingredients:\n",
      "- 1 pie crust (store-bought or homemade)\n",
      "- 1 onion, thinly sliced\n",
      "- 4 eggs\n",
      "- 1 cup heavy cream\n",
      "- 1 cup shredded Gruyere cheese\n",
      "- 6 strips of bacon, cooked and crumbled\n",
      "- 2 tablespoons butter\n",
      "- Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "1. Preheat your oven to 375°F (190°C).\n",
      "2. In a skillet, melt the butter over medium heat. Add the sliced onion and cook until caramelized, about 15-20 minutes. Set aside to cool.\n",
      "3. In a mixing bowl, whisk together the eggs and heavy cream. Season with salt and pepper.\n",
      "4. Place the pie crust in a pie dish and crimp the edges. Prick the bottom of the crust with a fork.\n",
      "5. Spread the caramelized onions evenly over the bottom of the pie crust. Sprinkle the shredded Gruyere cheese on top.\n",
      "6. Pour the egg mixture over the onions and cheese. Sprinkle the crumbled bacon on top.\n",
      "7. Bake the quiche in the preheated oven for 35-40 minutes, or until the center is set and the top is golden brown.\n",
      "8. Allow the quiche to cool for a few minutes before slicing and serving.\n",
      "\n",
      "Enjoy your delicious Quiche Lorraine made with the ingredients you have on hand!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise ChatOpenAI \n",
    "openai_set = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Use LLMChain to apply the prompt and model \n",
    "chain = LLMChain(llm=openai_set, prompt=p_template)\n",
    "\n",
    "# Run the chain with formatted input\n",
    "response = chain.run(ingredients='Onion, Eggs, Butter, Bacon')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build this out further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs_90\\AppData\\Local\\Temp\\ipykernel_18392\\2186144104.py:19: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = chatgpt(chat_prompt.format_prompt(ingredients=\"Curry, Apple, Potatoes, Onions, Carrot, Chicken\").to_messages())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend making a delicious Chicken Curry with Apple and Vegetables. Here's how you can make it:\n",
      "\n",
      "Ingredients:\n",
      "- 1 tablespoon oil\n",
      "- 1 onion, diced\n",
      "- 2 cloves of garlic, minced\n",
      "- 1 tablespoon curry powder\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon cumin\n",
      "- 1 teaspoon coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1 apple, peeled and diced\n",
      "- 2 potatoes, peeled and diced\n",
      "- 1 carrot, peeled and sliced\n",
      "- 2 chicken breasts, diced\n",
      "- 1 cup chicken broth\n",
      "- Salt and pepper to taste\n",
      "- Fresh cilantro for garnish\n",
      "\n",
      "Instructions:\n",
      "1. Heat the oil in a large pot over medium heat. Add the diced onion and garlic, and sauté until softened.\n",
      "2. Add the curry powder, turmeric, cumin, coriander, and paprika to the pot. Stir well to coat the onions and garlic in the spices.\n",
      "3. Add the diced apple, potatoes, carrot, and chicken to the pot. Stir to combine everything with the spices.\n",
      "4. Pour in the chicken broth and bring the mixture to a simmer. Cover the pot and let it cook for about 20-25 minutes, or until the chicken is cooked through and the vegetables are tender.\n",
      "5. Season with salt and pepper to taste.\n",
      "6. Serve the chicken curry over rice or with naan bread, and garnish with fresh cilantro.\n",
      "\n",
      "Enjoy your homemade Chicken Curry with Apple and Vegetables! It's a flavorful and comforting dish that makes for a satisfying meal.\n"
     ]
    }
   ],
   "source": [
    "chatgpt = ChatOpenAI(temperature=0)\n",
    "\n",
    "template_cooking = \"\"\"\n",
    "You are a fine dining chef. Recommend me one dish I can make with only the ingredients that I have, and explain how I can make it. \n",
    "The ingredients I have are: \n",
    "{ingredients}\n",
    "\"\"\"\n",
    "\n",
    "sys_msg_promt = SystemMessagePromptTemplate.from_template(template_cooking)\n",
    "\n",
    "# Human input\n",
    "human_template = \"{ingredients}\"\n",
    "human_msg_promt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "\n",
    "# input system message and human message into ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([sys_msg_promt, human_msg_promt])\n",
    "\n",
    "answer = chatgpt(chat_prompt.format_prompt(ingredients=\"Curry, Apple, Potatoes, Onions, Carrot, Chicken\").to_messages())\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few-Shot**\n",
    "\n",
    "You provide the machine with examples of outputs to guide it to output response in the way user wants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What are some interesting drinks that can be made with coffee?\",\n",
    "        \"answer\":\n",
    "        \"\"\"\n",
    "        There is a coffee in German called einspänner.\n",
    "        It is made by topping hot espresso with whipped cream. \n",
    "        It is not well known outside of Germany, although it did cath on in Korea for a while. \n",
    "        \"\"\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"question\": \"What are some interesting drinks that can be made with beer?\",\n",
    "        \"answer\": \n",
    "        \"\"\"\n",
    "        There is a drink in German called \"Diesel\".  \n",
    "        It is made by mixing beer with coca-cola. \n",
    "        it is not well known outside of Germany, although there are some German pubs that sell this in Sydney.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Creating a PromptTemplate for each example \n",
    "example_prompt = PromptTemplate(input_variables=[\"question\",\"answer\"], \n",
    "                                template=\"Question: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are some interesting drinks that can be made with coffee?\n",
      "Answer: \n",
      "        There is a coffee in German called einspänner.\n",
      "        It is made by topping hot espresso with whipped cream. \n",
      "        It is not well known outside of Germany, although it did cath on in Korea for a while. \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are some interesting drinks that can be made with coffee?\n",
      "Answer: \n",
      "        There is a coffee in German called einspänner.\n",
      "        It is made by topping hot espresso with whipped cream. \n",
      "        It is not well known outside of Germany, although it did cath on in Korea for a while. \n",
      "        \n",
      "\n",
      "Question: What are some interesting drinks that can be made with beer?\n",
      "Answer: \n",
      "        There is a drink in German called \"Diesel\".  \n",
      "        It is made by mixing beer with coca-cola. \n",
      "        it is not well known outside of Germany, although there are some German pubs that sell this in Sydney.\n",
      "        \n",
      "\n",
      "Question: What are some intersting drinks that can be made with strawberries?\n"
     ]
    }
   ],
   "source": [
    "# Set up Few-shot prompt template:\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input = \"What are some intersting drinks that can be made with strawberries?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "        One interesting drink that can be made with strawberries is a strawberry basil margarita. \n",
      "        It combines fresh strawberries, basil leaves, tequila, lime juice, and agave nectar for a refreshing and flavorful cocktail. \n",
      "        Another option is a strawberry mojito, which adds muddled strawberries to the classic mint, lime, rum, and soda water combination for a fruity twist on a traditional drink.\n"
     ]
    }
   ],
   "source": [
    "chatgpt = ChatOpenAI(temperature=0)\n",
    "\n",
    "formatted_prompt = prompt.format(input=\"What are some intersting drinks that can be made with strawberries?\")\n",
    "\n",
    "# Call/invoke the ChatOpenAI model with the formatted prompt\n",
    "response = chatgpt.invoke(formatted_prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few-Shot learning using EXAMPLE SELECTOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\",\"output\"],\n",
    "    template=\"input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "# These are examples of a pretend task of creating antonyms \n",
    "examples = [\n",
    "    {\"input\":\"Happy\", \"output\":\"Sad\"},\n",
    "    {\"input\":\"Exciting\", \"output\":\"Boring\"},\n",
    "    {\"input\":\"Long\", \"output\":\"Short\"},\n",
    "    {\"input\":\"Small\", \"output\":\"Large\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb\n",
    "%pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs_90\\AppData\\Local\\Temp\\ipykernel_18392\\779033454.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  OpenAIEmbeddings(),\n"
     ]
    }
   ],
   "source": [
    "# To produce outputs similar to the examples provided\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # List of examples (from above)\n",
    "    examples, \n",
    "    # Embedding (오픈AI Embedding을 써서 수치화 하기) class used to produce embeddings which are used to measure semantic similarity\n",
    "    OpenAIEmbeddings(), \n",
    "    # VectorStore class used to store the embeddings and do a similarity search over\n",
    "    Chroma, \n",
    "    # Number of examples to produce\n",
    "    k = 1\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector = example_selector,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix = \"Produce an antonym to the given word.\",\n",
    "    suffix = \"Input: {word}\\nOutput:\",\n",
    "    input_variables = [\"word\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produce an antonym to the given word.\n",
      "\n",
      "input: Happy\n",
      "Output: Sad\n",
      "\n",
      "input: Scary\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(similar_prompt.format(word = \"Scary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comforting\n"
     ]
    }
   ],
   "source": [
    "chatgpt = ChatOpenAI(temperature=0)\n",
    "\n",
    "query = \"Scary\"\n",
    "formatted_prompt = similar_prompt.format(word=query)\n",
    "\n",
    "print(chatgpt.invoke(formatted_prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Parser** 를 활용한 출력값 조정\n",
    "- Can fix the format of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions\n",
    "\n",
    "# Can manually assign instructions\n",
    "# format_instructions = \"Provide your recommendations as a single line, with items separated by commas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = \"Recommend 5 {topic} from the 2000s. {format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. The Dark Knight\\n2. Inception\\n3. Eternal Sunshine of the Spotless Mind\\n4. The Departed\\n5. Up'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = prompt.format(topic=\"movies\")\n",
    "output = model(_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. The Dark Knight\\n2. Inception\\n3. Eternal Sunshine of the Spotless Mind\\n4. The Departed\\n5. Up']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
