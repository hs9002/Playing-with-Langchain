{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall quality of your RAG will depend on how well you've tweaked the 'RETRIEVERS'. How well it understands the user query and how well it can extact correct information.\n",
    "\n",
    "# More advanced RAG \n",
    "\n",
    "## 1. Multi-Query Retriever\n",
    "\n",
    "The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever can mitigate some of the limitations of the distance-based retrieval and get a richer set of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pypdf\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.11.8-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n",
      "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.1.3)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Using cached chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.20.1-cp312-cp312-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.20.3)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.68.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.14.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.0.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.12-cp312-none-win_amd64.whl.metadata (42 kB)\n",
      "Collecting httpx>=0.27.0 (from chromadb)\n",
      "  Downloading httpx-0.28.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.0-cp312-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading propcache-0.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.18.0-cp312-cp312-win_amd64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.21->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.0-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: sympy in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.26.3)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.0-cp312-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-14.1-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 9.6 MB/s eta 0:00:00\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading chromadb-0.5.20-py3-none-any.whl (617 kB)\n",
      "   ---------------------------------------- 0.0/617.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 617.9/617.9 kB 11.5 MB/s eta 0:00:00\n",
      "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "Downloading aiohttp-3.11.8-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Downloading bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
      "Downloading grpcio-1.68.0-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.8/4.4 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.7/4.4 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading httpx-0.28.0-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.8.0-cp312-none-win_amd64.whl (206 kB)\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Downloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
      "Using cached langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading mmh3-5.0.1-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading onnxruntime-1.20.1-cp312-cp312-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.3 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/11.3 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.8/11.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.3 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.28.2-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl (159 kB)\n",
      "Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl (118 kB)\n",
      "Downloading orjson-3.10.12-cp312-none-win_amd64.whl (135 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Downloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 9.2 MB/s eta 0:00:00\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 7.8 MB/s eta 0:00:00\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading typer-0.14.0-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading propcache-0.2.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading protobuf-5.29.0-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading watchfiles-1.0.0-cp312-none-win_amd64.whl (285 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-14.1-cp312-cp312-win_amd64.whl (163 kB)\n",
      "Downloading yarl-1.18.0-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib, pypika\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.6-cp312-cp312-win_amd64.whl size=154973 sha256=f6a2605c349c1358fc824245a5679be98f7cdfb8df70c7e3073d87c1fc400ae0\n",
      "  Stored in directory: c:\\users\\hs_90\\appdata\\local\\pip\\cache\\wheels\\28\\29\\0e\\934c768c2e673547ec6e947e821346f4ed691a089fe046743f\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53834 sha256=3e5328894b998008f046b5ecebbed497c561ac1cc7c402251efeb0380d731f98\n",
      "  Stored in directory: c:\\users\\hs_90\\appdata\\local\\pip\\cache\\wheels\\d5\\3d\\69\\8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built chroma-hnswlib pypika\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, zipp, wrapt, websockets, websocket-client, tenacity, sniffio, shellingham, python-dotenv, pyreadline3, pyproject_hooks, pypdf, pydantic-core, pyasn1, protobuf, propcache, overrides, orjson, opentelemetry-util-http, oauthlib, multidict, mmh3, mdurl, jsonpointer, jiter, importlib-resources, httptools, h11, grpcio, greenlet, frozenlist, distro, click, chroma-hnswlib, cachetools, bcrypt, backoff, attrs, asgiref, annotated-types, aiohappyeyeballs, yarl, uvicorn, SQLAlchemy, rsa, requests-toolbelt, requests-oauthlib, pydantic, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, jsonpatch, importlib-metadata, humanfriendly, httpcore, googleapis-common-protos, deprecated, build, anyio, aiosignal, watchfiles, starlette, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpx, google-auth, coloredlogs, aiohttp, typer, opentelemetry-semantic-conventions, openai, onnxruntime, langsmith, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, opentelemetry-instrumentation-fastapi, langchain, chromadb\n",
      "Successfully installed SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.8 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2.post1 asgiref-3.8.1 attrs-24.2.0 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 cachetools-5.5.0 chroma-hnswlib-0.7.6 chromadb-0.5.20 click-8.1.7 coloredlogs-15.0.1 deprecated-1.2.15 distro-1.9.0 durationpy-0.9 fastapi-0.115.5 flatbuffers-24.3.25 frozenlist-1.5.0 google-auth-2.36.0 googleapis-common-protos-1.66.0 greenlet-3.1.1 grpcio-1.68.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.28.0 humanfriendly-10.0 importlib-metadata-8.5.0 importlib-resources-6.4.5 jiter-0.8.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-31.0.0 langchain-0.3.9 langchain-core-0.3.21 langchain-text-splitters-0.3.2 langsmith-0.1.147 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.0.1 monotonic-1.6 multidict-6.1.0 oauthlib-3.2.2 onnxruntime-1.20.1 openai-1.55.3 opentelemetry-api-1.28.2 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-sdk-1.28.2 opentelemetry-semantic-conventions-0.49b2 opentelemetry-util-http-0.49b2 orjson-3.10.12 overrides-7.7.0 posthog-3.7.4 propcache-0.2.0 protobuf-5.29.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.2 pydantic-core-2.27.1 pypdf-5.1.0 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 python-dotenv-1.0.1 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-13.9.4 rsa-4.9 shellingham-1.5.4 sniffio-1.3.1 starlette-0.41.3 tenacity-9.0.0 typer-0.14.0 uvicorn-0.32.1 watchfiles-1.0.0 websocket-client-1.8.0 websockets-14.1 wrapt-1.17.0 yarl-1.18.0 zipp-3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain pypdf chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/10.0 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.7/10.0 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.8/2.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 8.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.4/11.0 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 8.1 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.8/44.5 MB 10.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.7/44.5 MB 9.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 5.5/44.5 MB 9.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 7.3/44.5 MB 9.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 9.2/44.5 MB 9.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 11.3/44.5 MB 9.0 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 13.1/44.5 MB 9.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 14.9/44.5 MB 9.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 16.8/44.5 MB 9.1 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 18.9/44.5 MB 9.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 20.7/44.5 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 22.5/44.5 MB 9.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 24.6/44.5 MB 9.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 26.5/44.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 28.3/44.5 MB 9.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.1/44.5 MB 9.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 32.0/44.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.8/44.5 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.7/44.5 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.5/44.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.3/44.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.2/44.5 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 43.0/44.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 8.9 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 9.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.6/12.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.4/12.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.6/2.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 8.5 MB/s eta 0:00:00\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, tqdm, threadpoolctl, safetensors, regex, pyyaml, Pillow, numpy, joblib, idna, charset-normalizer, certifi, scipy, requests, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed Pillow-11.0.0 certifi-2024.8.30 charset-normalizer-3.4.0 huggingface-hub-0.26.3 idna-3.10 joblib-1.4.2 numpy-2.1.3 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.3.1 threadpoolctl-3.5.0 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.46.3 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/203.0 MB 8.3 MB/s eta 0:00:25\n",
      "    --------------------------------------- 3.7/203.0 MB 8.7 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 5.2/203.0 MB 8.6 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 7.1/203.0 MB 8.7 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 8.9/203.0 MB 8.7 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 11.0/203.0 MB 8.8 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 12.6/203.0 MB 8.7 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 14.4/203.0 MB 8.7 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 16.3/203.0 MB 8.7 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 18.1/203.0 MB 8.6 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 19.7/203.0 MB 8.6 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 21.5/203.0 MB 8.6 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 23.3/203.0 MB 8.6 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 25.2/203.0 MB 8.7 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 27.3/203.0 MB 8.7 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 29.1/203.0 MB 8.7 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 30.9/203.0 MB 8.7 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 32.8/203.0 MB 8.7 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 34.9/203.0 MB 8.7 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 36.4/203.0 MB 8.7 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 38.0/203.0 MB 8.7 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 39.8/203.0 MB 8.6 MB/s eta 0:00:19\n",
      "   -------- ------------------------------- 41.7/203.0 MB 8.6 MB/s eta 0:00:19\n",
      "   -------- ------------------------------- 43.5/203.0 MB 8.7 MB/s eta 0:00:19\n",
      "   -------- ------------------------------- 45.4/203.0 MB 8.7 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 47.2/203.0 MB 8.7 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 49.3/203.0 MB 8.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 51.1/203.0 MB 8.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 53.0/203.0 MB 8.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 8.7 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 56.6/203.0 MB 8.7 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 58.5/203.0 MB 8.7 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 60.3/203.0 MB 8.8 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 62.4/203.0 MB 8.8 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 64.2/203.0 MB 8.8 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 66.1/203.0 MB 8.8 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 68.2/203.0 MB 8.8 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 70.0/203.0 MB 8.8 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 71.8/203.0 MB 8.8 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 73.7/203.0 MB 8.8 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 75.8/203.0 MB 8.8 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 77.6/203.0 MB 8.9 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 79.4/203.0 MB 8.9 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 81.5/203.0 MB 8.8 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 83.4/203.0 MB 8.9 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 85.5/203.0 MB 8.9 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 87.3/203.0 MB 8.9 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 89.4/203.0 MB 8.9 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 91.0/203.0 MB 8.9 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 92.8/203.0 MB 8.9 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 94.9/203.0 MB 8.9 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 96.7/203.0 MB 8.9 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 98.6/203.0 MB 8.9 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 100.4/203.0 MB 8.9 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 102.2/203.0 MB 8.9 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 104.1/203.0 MB 8.9 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 106.2/203.0 MB 8.9 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 108.0/203.0 MB 8.9 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 110.1/203.0 MB 8.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 111.9/203.0 MB 8.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 8.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 115.6/203.0 MB 8.9 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 117.4/203.0 MB 8.9 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 119.3/203.0 MB 8.9 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 121.1/203.0 MB 8.9 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 123.2/203.0 MB 8.9 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 125.0/203.0 MB 8.9 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 126.9/203.0 MB 8.9 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 128.7/203.0 MB 8.9 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 130.5/203.0 MB 8.9 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 132.4/203.0 MB 8.9 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 134.5/203.0 MB 8.9 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 136.1/203.0 MB 8.9 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 138.1/203.0 MB 8.9 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 140.2/203.0 MB 8.9 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 141.8/203.0 MB 8.9 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 143.7/203.0 MB 8.9 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 145.8/203.0 MB 8.9 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 147.6/203.0 MB 8.9 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 149.4/203.0 MB 8.9 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 151.3/203.0 MB 9.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 153.1/203.0 MB 8.9 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 154.9/203.0 MB 8.9 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 156.8/203.0 MB 8.9 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 158.6/203.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 160.4/203.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 162.3/203.0 MB 8.9 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 164.1/203.0 MB 8.9 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 166.2/203.0 MB 8.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 168.0/203.0 MB 8.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 169.9/203.0 MB 8.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 171.7/203.0 MB 8.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 173.5/203.0 MB 9.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 175.4/203.0 MB 8.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 177.5/203.0 MB 9.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 179.3/203.0 MB 9.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 181.1/203.0 MB 9.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 183.0/203.0 MB 9.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 184.8/203.0 MB 9.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 186.6/203.0 MB 9.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 188.5/203.0 MB 9.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 190.3/203.0 MB 9.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 192.4/203.0 MB 9.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 194.2/203.0 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 196.1/203.0 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 197.9/203.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.8/203.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  201.6/203.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 203.0/203.0 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 8.8 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 8.5 MB/s eta 0:00:00\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 setuptools-75.6.0 sympy-1.13.1 torch-2.5.1 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "torch supported version of Python was 3.12 but I was using 3.13.\n",
    "Installed Python 3.12 and switched to it to get this working.\n",
    "Once that was done, I reran the install for sentence-transformers\n",
    "\"\"\"\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n",
      "  Downloading SQLAlchemy-2.0.35-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (3.11.8)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.8 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.3.9)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.3.21)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.1.147)\n",
      "Collecting numpy<2,>=1.26.2 (from langchain-community)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (2.10.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.28.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (2.27.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.6/2.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 7.8 MB/s eta 0:00:00\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.6/15.5 MB 9.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.7/15.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.5 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.8/15.5 MB 8.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.9/15.5 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.7/15.5 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.4/15.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 8.6 MB/s eta 0:00:00\n",
      "Using cached pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 8.3 MB/s eta 0:00:00\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: SQLAlchemy, numpy, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.36\n",
      "    Uninstalling SQLAlchemy-2.0.36:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.36\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.8 marshmallow-3.23.1 mypy-extensions-1.0.0 numpy-1.26.4 pydantic-settings-2.6.1 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hs_90\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hs_90\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hs_90\\.cache\\huggingface\\hub\\models--jhgan--ko-sbert-nli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# libraries for multiquery retriever\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# loading news article \n",
    "loader = WebBaseLoader(\"https://n.news.naver.com/mnews/article/003/0012317114?sid=105\")\n",
    "data = loader.load()\n",
    "\n",
    "# split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorDB = Chroma.from_documents(documents=splits, embedding=ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs_90\\AppData\\Local\\Temp\\ipykernel_11000\\3819617527.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0,\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "question = \"  S24  ?\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 openai_api_key=\"YOUR API KEY HERE\")\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorDB.as_retriever(), \n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for queries\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs_90\\AppData\\Local\\Temp\\ipykernel_11000\\511090460.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1.   S24    ?', '2.   S24   ?', '3.   S24    ?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many relevant generated queries?\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/003/0012317114?sid=105', 'title': \" D-4,   AI 'S24'  \"}, page_content='S25S25 ,   \\n    S25, S25      IT  29() .  IT  (Roland Quandt  S25, '),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/003/0012317114?sid=105', 'title': \" D-4,   AI 'S24'  \"}, page_content='\\n\\n\\n \\n\\n  \\n\\n\\n\\n1\\n\\n\\n\\n2\\n\\n\\n\\n3\\n\\n\\n\\n4\\n\\n\\n\\n5\\n\\n\\n\\n\\n\\n\\n\\nSNS \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n, 17( )   S24     ,   10    ,     AI   '),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/003/0012317114?sid=105', 'title': \" D-4,   AI 'S24'  \"}, page_content='  ,  ,  ,    4   ,  ,  ,    4   .       ,  ,    .   ,   10  19         .,  (256GB )     . 256GB     '),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/003/0012317114?sid=105', 'title': \" D-4,   AI 'S24'  \"}, page_content=\" D-4,   AI 'S24'  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nNAVER\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\nIT/\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTV\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/003/0012317114?sid=105', 'title': \" D-4,   AI 'S24'  \"}, page_content='  AI       .            IT ()   . 14            S24    .\"   \"   AI    \\' AI\\''),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/003/0012317114?sid=105', 'title': \" D-4,   AI 'S24'  \"}, page_content='   1155000, 1353000    . 512GB    22000  1298000, 1496000  .     . 256GB 99000  1698400, 512GB 121000  1841400 .   19 25 7        ,   ,   FE    .')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parent document retriever\n",
    "\n",
    "Generally, when user inputs a question a relevant chunk will be selected from the VectorStore and passed to LLM. What Parent document retriever does is to call on the actual parent document that contains the chunk. This helps to improve the context - while the other chunks within the parent may not be highly relevant it may provide good contextual info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "from langchain.storage import InMemoryStore    # stores the key-value pairs in dictionary form\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cryptography in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (44.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.12->cryptography) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(\"C:/python_proj/pdf_docs/am-mid-year-outlook-2024.pdf\"),\n",
    "    PyPDFLoader(\"C:/python_proj/pdf_docs/Global Strategy Paper_ Updating our long-term return forecast.pdf\")\n",
    "]\n",
    "\n",
    "# list\n",
    "docs = []\n",
    "\n",
    "# collect the pdfs into loaders object and get the text chunks using load_and_split and store into docs\n",
    "for loader in loaders: \n",
    "    docs.extend(loader.load_and_split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hs_90\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare child splitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# Vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", # where to store the chroma vectorDB\n",
    "    embedding_function=ko_embedding # using ko_embedding function declared above\n",
    ")\n",
    "\n",
    "# Storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through add_documents we will add the list of chunks from the two PDF files. \n",
    "# so we are adding this to retriever which we declared with ParentDocumentRetriever.\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child document\n",
    "sub_docs = vectorstore.similarity_search(\"AI outlook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 467 \n",
      "\n",
      "\n",
      "AI winners and to understand and apply the technology \n",
      "themselves. A combination of investing in and with artificial \n",
      "intelligencetwo distinct but interrelated approachesmay \n",
      "drive long-term outperformance, but the journey is likely to \n",
      "be complex and continuously evolving. As AI models become \n",
      "more sophisticated and complex, the reliance on high-quality \n",
      "data sets is among the highest priorities. The differentiating \n",
      "factor will be the meticulous management of data, including\n"
     ]
    }
   ],
   "source": [
    "print(\"Length: {} \\n\".format(len(sub_docs[0].page_content)))\n",
    "print(sub_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs_90\\AppData\\Local\\Temp\\ipykernel_17132\\4192869999.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(\"AI outlook\")\n"
     ]
    }
   ],
   "source": [
    "# lets compare. this is the one with Parent document\n",
    "retrieved_docs = retriever.get_relevant_documents(\"AI outlook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 3719 \n",
      "\n",
      "Investment Considerations\n",
      "TAILWINDS AND HEADWINDS: INVESTING IN MEGATRENDS\n",
      "Artificial Intelligence: Data is the \n",
      "Differentiator\n",
      "The advent and rapid adoption of generative AI is creating \n",
      "opportunities across asset classes and industries. Investors \n",
      "will need clear strategies to find the next generation of \n",
      "AI winners and to understand and apply the technology \n",
      "themselves. A combination of investing in and with artificial \n",
      "intelligencetwo distinct but interrelated approachesmay \n",
      "drive long-term outperformance, but the journey is likely to \n",
      "be complex and continuously evolving. As AI models become \n",
      "more sophisticated and complex, the reliance on high-quality \n",
      "data sets is among the highest priorities. The differentiating \n",
      "factor will be the meticulous management of data, including \n",
      "where the data originates, storage infrastructure, cleansing \n",
      "protocols, and measures in place to keep it secure. Companies \n",
      "limits within the context of generative AI capabilities are \n",
      "likely to be determined by the quality of datasets. Adequate \n",
      "infrastructure and the intellectual capital to understand the \n",
      "economic rationale and conviction behind AI models will also \n",
      "be key. \n",
      "Advancing Sustainable Growth\n",
      "The world remains, in our view, in the early innings of a multi-\n",
      "decade shift toward sustainable, inclusive growth. The vast \n",
      "amount of investment needed on the journey makes green, \n",
      "social and sustainability (GSS) bonds increasingly important. \n",
      "In the first quarter of 2024, GSS bond issuance totaled $272 \n",
      "billion. This is expected to grow to a record $1 trillion in \n",
      "2024.9 Green bond issuance alone totaled $195 billion in the \n",
      "first three months of the yearthe strongest first quarter on \n",
      "record. Looking ahead, we expect green bond issuance to grow \n",
      "in Asia and the Middle East, driven by the rise of sustainable \n",
      "projects, such as green buildings and the need for climate \n",
      "change adaptation measures. Currently, GSS bonds make up \n",
      "only 5% of emerging market sovereign external debt, with \n",
      "the corporate bond market reflecting a similar trend. As \n",
      "green bonds and the broader GSS bond market matures, we \n",
      "expect more investors to diversify existing EM fixed income \n",
      "allocations toward this segment. \n",
      "China Comeback?\n",
      "Chinas economic headwinds persist and conditions in the \n",
      "property sector look likely to determine the overall rate of \n",
      "recovery. High debt levels, geopolitical trade tensions, and \n",
      "aging demographics may mean a recovery in this cycle is more \n",
      "challenging and likely longer drawn. Chinas policymakers \n",
      "have not sat on the sidelines. The intensity, acceleration, \n",
      "and scope of fiscal, monetary and property policy measures \n",
      "increased in the first half of 2024. Chinese equities rallied in \n",
      "response, helped by better-than-expected economic growth. \n",
      "More investors may review their tactical stance on Chinese \n",
      "equities in the months ahead, particularly as Chinese stocks \n",
      "still trade at historically high discounts to global equity peers. \n",
      "We remain focused on the long-term fundamentals of Chinese \n",
      "companies and see opportunities in the areas of advanced \n",
      "manufacturing and technology innovation, which is broad \n",
      "based and benefiting healthcare and IT sectors. A focus on \n",
      "businesses with more stable free cash flows and shareholder \n",
      "returns may prove rewarding over the coming quarters. \n",
      "Recently announced US tariffs on Chinese electric vehicles, \n",
      "solar cells, and lithium-ion batteriesareas identified by \n",
      "China policymakers as drivers of high-quality growth\n",
      "highlight the importance of monitoring international trade \n",
      "policy and seeking to avoid companies that may get caught in \n",
      "the crosshairs of geopolitics.\n",
      "Goldman Sachs Asset Management\n",
      "Asset Management Outlook 2024     |    14\n"
     ]
    }
   ],
   "source": [
    "print(\"Length: {} \\n\".format(len(retrieved_docs[0].page_content)))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the full chunk is too long we can introduce parent_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size = 800)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size = 300) # this should be smaller than parent\n",
    "\n",
    "# Vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=ko_embedding\n",
    ")\n",
    "\n",
    "# Storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add docs using add_documents\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets have a look at how many keys are in the ParentDocumentRetriever we saved\n",
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be complex and continuously evolving. As AI models become \n",
      "more sophisticated and complex, the reliance on high-quality \n",
      "data sets is among the highest priorities. The differentiating \n",
      "factor will be the meticulous management of data, including\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"AI outlook\")\n",
    "\n",
    "print(sub_docs[0].page_content)\n",
    "\n",
    "# output is a chunk from the child \n",
    "# can check the length (to check it is around what we set for the child chunk size)\n",
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Considerations\n",
      "TAILWINDS AND HEADWINDS: INVESTING IN MEGATRENDS\n",
      "Artificial Intelligence: Data is the \n",
      "Differentiator\n",
      "The advent and rapid adoption of generative AI is creating \n",
      "opportunities across asset classes and industries. Investors \n",
      "will need clear strategies to find the next generation of \n",
      "AI winners and to understand and apply the technology \n",
      "themselves. A combination of investing in and with artificial \n",
      "intelligencetwo distinct but interrelated approachesmay \n",
      "drive long-term outperformance, but the journey is likely to \n",
      "be complex and continuously evolving. As AI models become \n",
      "more sophisticated and complex, the reliance on high-quality \n",
      "data sets is among the highest priorities. The differentiating \n",
      "factor will be the meticulous management of data, including\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarly, to get from parent document\n",
    "retrieved_docs = retriever.get_relevant_documents(\"AI outlook\")\n",
    "print(retrieved_docs[0].page_content)\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Querying Retriever\n",
    "\n",
    "Uses the document's metadata to filter content to generate more accurate response. \n",
    "Useful when you are using excel or csv documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lark in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# lets create our data for this exercise. we'll insert some documents into the docs object.\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content = \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993,\n",
    "                  \"rating\": 7.7,\n",
    "                  \"genre\": \"science fiction\"}\n",
    "    ), \n",
    "    Document(\n",
    "        page_content = \"Leo Dicaprio gets lost in a dream within a drean within a dream within a...\",\n",
    "        metadata={\"year\": 2010,\n",
    "                  \"rating\": 8.2,\n",
    "                  \"director\": \"Christopher Nolan\",\n",
    "                  \"genre\": \"science fiction\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content = \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019,\n",
    "                  \"rating\": 8.3,\n",
    "                  \"director\": \"Greta Gerwig\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# embedding\n",
    "vectorstore = Chroma.from_documents(docs, ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs_90\\AppData\\Local\\Temp\\ipykernel_17716\\2621007879.py:32: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0,\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Defining the metadata from above\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        # where possible, it is good to define the range of attributes (like here with the list of genre), otherwise it could hallucinate\n",
    "        description=\"The genre of the movie. One of ['science fiction','comedy','drama','thriller','horror','action']\",\n",
    "        type=\"string\"\n",
    "    ), \n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"Year the movie was released.\",\n",
    "        type=\"integer\"\n",
    "    ), \n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"Name of the movie director.\",\n",
    "        type=\"string\"\n",
    "    ), \n",
    "    AttributeInfo(\n",
    "        name=\"rating\",\n",
    "        description=\"A 1-10 rating for the movie\",\n",
    "        type=\"float\"\n",
    "    )\n",
    "]\n",
    "\n",
    "document_content_description = \"Brief summary of movie\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 openai_api_key = \"YOUR API KEY HERE\")\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'director': 'Christopher Nolan', 'genre': 'science fiction', 'rating': 8.2, 'year': 2010}, page_content='Leo Dicaprio gets lost in a dream within a drean within a dream within a...'), Document(metadata={'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019}, page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them')]\n"
     ]
    }
   ],
   "source": [
    "test = retriever.invoke(\"What are some movies rated 8.0 or higher.\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time-weighted Vector Retriever \n",
    "\n",
    "Maintains the relevance of the response by giving higher weight to more recent documents.\n",
    "It penalises older documents:\n",
    "\n",
    "**semantic_similarity + (1.0 - decay_rate)^age_of_info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\hs_90\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\hs_90\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl (13.8 MB)\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.8/13.8 MB 9.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.7/13.8 MB 9.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.2/13.8 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.3/13.8 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.2/13.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.7/13.8 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.8 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.8/13.8 MB 8.5 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hs_90\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# embedding model\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intiialise the vectorstore as empty\n",
    "\"\"\"\n",
    "The embedding_size: Here we are using ko-sbert-nli\n",
    "If you go to huggingface website you will find that this sentence transformers model maps \n",
    "sentences and paragraphs to a 768 dimensional dense vector space and can be used\n",
    "for tasks like clustering or semantic search.\n",
    "\"\"\"\n",
    "embedding_size = 768  \n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(ko_embedding,\n",
    "                    index,\n",
    "                    InMemoryDocstore({}),\n",
    "                    {})\n",
    "\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    decay_rate=0.1,   # start small initially\n",
    "    k=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6fa63624-b572-43cb-8814-c9af49eef063']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Add a document - marked with yesterday's date\n",
    "retriever.add_documents(\n",
    "    [Document(page_content = \"English is fun.\",\n",
    "              metadata = {\"last_accessed_at\":yesterday})]\n",
    ")\n",
    "\n",
    "# Add new document right after the one above\n",
    "retriever.add_documents([Document(page_content=\"Korean is fun.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hs_90\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\retrievers\\time_weighted_retriever.py:83: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'last_accessed_at': datetime.datetime(2024, 11, 30, 15, 24, 25, 272318), 'created_at': datetime.datetime(2024, 12, 1, 15, 24, 25, 272318), 'buffer_idx': 0}, page_content='English is powerful.'), 0.3660145449382758), (Document(metadata={'last_accessed_at': datetime.datetime(2024, 11, 30, 15, 25, 17, 316496), 'created_at': datetime.datetime(2024, 12, 1, 15, 25, 17, 316496), 'buffer_idx': 2}, page_content='English is fun.'), 0.3535781900544692), (Document(metadata={'last_accessed_at': datetime.datetime(2024, 11, 30, 15, 25, 56, 846981), 'created_at': datetime.datetime(2024, 12, 1, 15, 25, 56, 846981), 'buffer_idx': 4}, page_content='English is fun.'), 0.3535781900544692), (Document(metadata={'last_accessed_at': datetime.datetime(2024, 12, 1, 15, 24, 25, 312796), 'created_at': datetime.datetime(2024, 12, 1, 15, 24, 25, 312796), 'buffer_idx': 1}, page_content='Korean is powerful.'), 0.02477444043115673), (Document(metadata={'last_accessed_at': datetime.datetime(2024, 12, 1, 15, 25, 17, 361787), 'created_at': datetime.datetime(2024, 12, 1, 15, 25, 17, 361787), 'buffer_idx': 3}, page_content='Korean is fun.'), -0.01853009771463454), (Document(metadata={'last_accessed_at': datetime.datetime(2024, 12, 1, 15, 25, 56, 890058), 'created_at': datetime.datetime(2024, 12, 1, 15, 25, 56, 890058), 'buffer_idx': 5}, page_content='Korean is fun.'), -0.01853009771463454)]\n",
      "  docs_and_scores = self.vectorstore.similarity_search_with_relevance_scores(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'last_accessed_at': datetime.datetime(2024, 12, 1, 15, 26, 45, 781668), 'created_at': datetime.datetime(2024, 12, 1, 15, 24, 25, 312796), 'buffer_idx': 1}, page_content='Korean is powerful.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"I enjoy English.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
